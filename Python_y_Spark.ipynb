{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN92TcZqOG7L6YrtSNNlA06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EderLara/Fundamentos-Big-Data/blob/main/Python_y_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-DarX3kE_QdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Explicación del Código:\n",
        "  Se crea una sesión de Spark.\n",
        "  Se lee el conjunto de datos desde HDFS utilizando spark.read.csv().\n",
        "  Se utiliza la función lag() para acceder a la página anterior visitada por cada usuario.\n",
        "  Se utiliza la función groupBy() y agg(count(\"*\")) para contar el número de veces que se visita cada secuencia de páginas.\n",
        "  Se muestran los resultados utilizando show().\n",
        "  Se detiene la sesión de Spark.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Kg5VvHXu_X5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmM9cCqW-7bH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lag, col, count\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"AnalisisRegistrosWeb\").getOrCreate()\n",
        "\n",
        "# Leer el conjunto de datos desde HDFS\n",
        "registros_df = spark.read.csv(\"hdfs:///registros_web.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Identificar patrones de navegación\n",
        "patrones_df = registros_df.withColumn(\n",
        "    \"pagina_anterior\", lag(\"pagina_visitada\", 1, \"inicio\").over(partitionBy=\"usuario_id\", orderBy=\"fecha_acceso\")\n",
        ")\n",
        "\n",
        "# Contar el número de veces que se visita cada secuencia de páginas\n",
        "secuencias_df = patrones_df.groupBy(\"pagina_anterior\", \"pagina_visitada\").agg(count(\"*\").alias(\"conteo\"))\n",
        "\n",
        "# Mostrar los resultados\n",
        "secuencias_df.show()\n",
        "\n",
        "# Detener la sesión de Spark\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Ejemplo 1: Análisis de Datos de Ventas desde una Base de Datos SQL:\n",
        "\n",
        "Escenario:\n",
        "Tenemos datos de ventas almacenados en una base de datos MySQL.\n",
        "Queremos analizar estos datos utilizando Spark para identificar patrones de ventas y tendencias.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "OGWV842U_8DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"AnalisisVentasSQL\").getOrCreate()\n",
        "\n",
        "# Configuración de la conexión a la base de datos MySQL\n",
        "jdbc_url = \"jdbc:mysql://tu_servidor:tu_puerto/tu_base_de_datos\"\n",
        "jdbc_properties = {\n",
        "    \"user\": \"tu_usuario\",\n",
        "    \"password\": \"tu_contraseña\",\n",
        "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Leer los datos de la tabla de ventas\n",
        "ventas_df = spark.read.jdbc(url=jdbc_url, table=\"ventas\", properties=jdbc_properties)\n",
        "\n",
        "# Realizar análisis de ventas (ejemplo: ventas totales por producto)\n",
        "ventas_por_producto_df = ventas_df.groupBy(\"producto\").sum(\"monto\").withColumnRenamed(\"sum(monto)\", \"ventas_totales\")\n",
        "\n",
        "# Mostrar los resultados\n",
        "ventas_por_producto_df.show()\n",
        "\n",
        "# Detener la sesión de Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "cRmLHOzf_vUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract, to_timestamp\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"AnalisisLogsServidor\").getOrCreate()\n",
        "\n",
        "# Leer los registros de logs desde archivos de texto\n",
        "logs_df = spark.read.text(\"hdfs:///logs_servidor/*.log\")\n",
        "\n",
        "# Extraer información relevante utilizando expresiones regulares\n",
        "logs_df = logs_df.withColumn(\"fecha_hora\", regexp_extract(\"value\", r\"\\[(.*?)\\]\", 1))\n",
        "logs_df = logs_df.withColumn(\"url\", regexp_extract(\"value\", r\"\\\"GET (.*?) HTTP\\\"\", 1))\n",
        "logs_df = logs_df.withColumn(\"estado\", regexp_extract(\"value\", r\"\\\"HTTP/1.1 (\\d+)\", 1))\n",
        "\n",
        "# Convertir la columna de fecha y hora a un tipo timestamp\n",
        "logs_df = logs_df.withColumn(\"fecha_hora\", to_timestamp(\"fecha_hora\", \"dd/MMM/yyyy:HH:mm:ss Z\"))\n",
        "\n",
        "# Realizar análisis (ejemplo: contar el número de accesos por URL)\n",
        "accesos_por_url_df = logs_df.groupBy(\"url\").count().withColumnRenamed(\"count\", \"num_accesos\")\n",
        "\n",
        "# Mostrar los resultados\n",
        "accesos_por_url_df.show()\n",
        "\n",
        "# Detener la sesión de Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "o4QmFWc2AsbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper, col\n",
        "\n",
        "# Crear una sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"ETLClientes\").getOrCreate()\n",
        "\n",
        "# 1. Extracción: Leer datos desde el archivo CSV\n",
        "clientes_df = spark.read.csv(\"clientes.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# 2. Transformación: Limpiar y transformar los datos\n",
        "# Convertir nombres y apellidos a mayúsculas\n",
        "clientes_df = clientes_df.withColumn(\"nombre\", upper(col(\"nombre\")))\n",
        "clientes_df = clientes_df.withColumn(\"apellido\", upper(col(\"apellido\")))\n",
        "\n",
        "# Filtrar clientes activos\n",
        "clientes_activos_df = clientes_df.filter(col(\"activo\") == True)\n",
        "\n",
        "# 3. Carga: Escribir los datos transformados en una tabla de MySQL\n",
        "jdbc_url = \"jdbc:mysql://tu_servidor:tu_puerto/tu_base_de_datos\"\n",
        "jdbc_properties = {\n",
        "    \"user\": \"tu_usuario\",\n",
        "    \"password\": \"tu_contraseña\",\n",
        "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "clientes_activos_df.write.jdbc(url=jdbc_url, table=\"clientes_limpios\", mode=\"overwrite\", properties=jdbc_properties)\n",
        "\n",
        "# Detener la sesión de Spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "gTihBNq3DwVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "Extracción:\n",
        "spark.read.csv(\"clientes.csv\", header=True, inferSchema=True): Lee el archivo CSV y crea un DataFrame de Spark.\n",
        "\n",
        "Transformación:\n",
        "clientes_df.withColumn(\"nombre\", upper(col(\"nombre\"))): Convierte la columna \"nombre\" a mayúsculas.\n",
        "clientes_df.withColumn(\"apellido\", upper(col(\"apellido\"))): Convierte la columna \"apellido\" a mayúsculas.\n",
        "clientes_df.filter(col(\"activo\") == True): Filtra solo los clientes activos.\n",
        "\n",
        "Carga:\n",
        "clientes_activos_df.write.jdbc(...): Escribe el DataFrame transformado en la tabla \"clientes_limpios\" de MySQL.\n",
        "mode=\"overwrite\": Sobreescribe la tabla si ya existe.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gFUdEhEHD3kN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8jn2hRPEAak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}